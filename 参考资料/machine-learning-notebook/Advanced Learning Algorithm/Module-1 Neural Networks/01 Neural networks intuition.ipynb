{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98eaaa8f",
   "metadata": {},
   "source": [
    "# <center>2 Advanced Learning Algorithm</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314e64d",
   "metadata": {},
   "source": [
    "## <center>2.1 Neural Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3e93c",
   "metadata": {},
   "source": [
    "### 2.1.1 Neural Networks Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea590f",
   "metadata": {},
   "source": [
    "#### 2.1.1.1 Wecome!\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-01-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Welcome to Course 2 of this machine learning specialization. In this course, you'll learn about **neural networks**, also called deep learning algorithms, as well as **decision trees**. These are some of the most powerful and widely used machine learning algorithms and you'd get to implement them and get them to work for yourself. One of the things you see also in this course is practical advice on how to build machine learning systems. This part of the material is quite unique to this course. When you're building a practical machine learning system, there are a lot of decisions you have to make, such as should you spend more time collecting data or should you buy a much bigger GPU to build a much bigger neural network? *Even today, when I visit a leading tech company and talk to the team working there on a machine learning application, unfortunately, sometimes I look at what they've been doing for the last six months and go, gee, someone could have told you maybe even six months ago that that approach wasn't going to work that well.* With some of the tips that you learn in this course, I hope that you'll be one or the ones to not waste those six months, but instead, be able to make more systematic and better decisions about how to build practical working machine learning applications. With that, let's dive in. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-01-02.png' width=70%/>\n",
    "</div>\n",
    "\n",
    " In detail, this is what you see in the four weeks of this course. In Week 1, we'll go over neural networks and how to carry out inference or prediction. **If you were to go to the Internet and download the parameters of a neural network that someone else had trained and whose parameters that posted on the Internet, then to use that neural network to make predictions would be called inference**, and you learned how neural networks work, and how to do inference in this week. Next week, you'll learn how to train your own neural network. In particular, if you have a training set of labeled examples, X and Y, how do you train the parameters of a neural network for yourself? In the third week, we'll then go into practical advice for building machine learning systems and I'll share with you some tips that I think even highly paid engineers building machine learning systems very successfully today don't really always manage to consistently apply and I think that will help you build systems yourself efficiently and quickly. Then in the final week of this course, you learn about decision trees. While decision trees don't get as much buzz in the media, there's local less hype about decision trees compared to neural networks. They are also one of the widely used and very powerful learning algorithms that I think there's a good chance you end up using yourself if you end up building an application. With that, let's jump into neural networks and we're going to start by taking a quick look at how the human brain, that is how the biological brain works. Let's go on to the next video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cebef",
   "metadata": {},
   "source": [
    "#### 2.1.1.2 Neurons and the brain\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-02-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "When neural networks were first invented many decades ago, the original motivation was to write software that could **mimic how the human brain or how the biological brain learns and thinks**. Even though today, neural networks, sometimes also called artificial neural networks, have become very different than how any of us might think about how the brain actually works and learns. Some of the biological motivations still remain in the way we think about artificial neural networks or computer neural networks today. Let's start by taking a look at how the brain works and how that relates to neural networks. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-02-02.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "The human brain, or maybe more generally, the biological brain demonstrates a higher level or more capable level of intelligence and anything else would be on the bill so far. So **neural networks has started with the motivation of trying to build software to mimic the brain**. Work in neural networks had started back in the 1950s, and then it fell out of favor for a while. Then in the 1980s and early 1990s, they gained in popularity again and **showed tremendous traction in some applications like handwritten digit recognition**, which were used even backed then to read postal codes for writing mail and for reading dollar figures in handwritten checks. But then it fell out of favor again in the late 1990s. It was from about 2005 that it enjoyed a resurgence and also became re-branded little bit with deep learning. One of the things that surprised me back then was deep learning and neural networks meant very similar things. But maybe under-appreciated at the time that the term deep learning, just sounds much better because it's deep and this learning. So that turned out to be the brand that took off in the last decade or decade and a half. Since then, neural networks have revolutionized application area after application area. I think the first application area that modern neural networks or deep learning, had a huge impact on was probably **speech recognition**, where we started to see much better speech recognition systems due to modern deep learning and authors such as Li Deng and Geoff Hinton were instrumental to this, and then it started to make inroads into **computer vision**. Sometimes people still speak of the ImageNet moments in 2012, and that was maybe a bigger splash where then caught their imagination and had a big impact on computer vision. Then the next few years, it made us inroads into texts or into **natural language processing**, and so on and so forth. Now, neural networks are used in everything from climate change to medical imaging to online advertising to prouduct recommendations and really lots of application areas of machine learning now use neural networks. **Even though today's neural networks have almost nothing to do with how the brain learns, there was the early motivation of trying to build software to mimic the brain**. So how does the brain work?\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-02-03.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "*Here's a diagram illustrating what neurons in a brain look like. All of human thought is from neurons like this in your brain and mine, sending electrical impulses and sometimes forming new connections of other neurons. Given a neuron like this one, it has a number of inputs where it receives electrical impulses from other neurons, and then this neuron that I've circled carries out some computations and will then send this outputs to other neurons by this electrical impulses, and this upper neuron's output in turn becomes the input to this neuron down below, which again aggregates inputs from multiple other neurons to then maybe send its own output, to yet other neurons, and this is the stuff of which human thought is made.* \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-02-04.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "*Here's a simplified diagram of a biological neuron. A neuron comprises a cell body shown here on the left, and if you have taken a class in biology, you may recognize this to be the nucleus of the neuron. As we saw on the previous slide, the neuron has different inputs. In a biological neuron, the input wires are called the dendrites, and it then occasionally sends electrical impulses to other neurons via the output wire, which is called the axon. Don't worry about these biological terms. If you saw them in a biology class, you may remember them, but you don't really need to memorize any of these terms for the purpose of building artificial neural networks. But this biological neuron may then send electrical impulses that become the input to another neuron.* So **the artificial neural network uses a very simplified Mathematical model of what a biological neuron does**. I'm going to draw a little circle here to denote a single neuron. What a neuron does is it takes some inputs, one or more inputs, which are just numbers. It does some computation and it outputs some other number, which then could be an input to a second neuron, shown here on the right. When you're building an artificial neural network or deep learning algorithm, rather than building one neuron at a time, you often want to simulate many such neurons at the same time. In this diagram, I'm drawing three neurons. What these neurons do collectively is **input a few numbers, carry out some computation, and output some other numbers**. Now, at this point, I'd like to give one big caveat, which is that even though I made a loose analogy between biological neurons and artificial neurons, I think that today we have almost no idea how the human brain works. *In fact, every few years, neuroscientists make some fundamental breakthrough about how the brain works. I think we'll continue to do so for the foreseeable future. That to me is a sign that there are many breakthroughs that are yet to be discovered about how the brain actually works, and thus attempts to blindly mimic what we know of the human brain today, which is frankly very little, probably won't get us that far toward building raw intelligence. Certainly not with our current level of knowledge in neuroscience. Having said that, even with these extremely simplified models of a neuron, which we'll talk about, we'll be able to build really powerful deep learning algorithms. So as you go deeper into neural networks and into deep learning, even though the origins were biologically motivated, don't take the biological motivation too seriously. In fact, those of us that do research in deep learning have shifted away from looking to biological motivation that much. But instead, they're just using engineering principles to figure out how to build algorithms that are more effective. But I think it might still be fun to speculate and think about how biological neurons work every now and then.* \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-02-05.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "The ideas of neural networks have been around for many decades. A few people have asked me, \"Hey Andrew, why now? Why is it that only in the last handful of years that neural networks have really taken off?\" This is a picture I draw for them when I'm asked that question and that maybe you could draw for others as well if they ask you that question. Let me plot on the horizontal axis **the amount of data you have for a problem**, and on the vertical axis, the **performance or the accuracy of a learning algorithm applied to that problem**. Over the last couple of decades, with the rise of the Internet, the rise of mobile phones, the digitalization of our society, the amount of data we have for a lot of applications has steadily marched to the right. Lot of records that used to be on paper, such as if you order something rather than it being on a piece of paper, there's much more likely to be a digital record. Your health record, if you see a doctor, is much more likely to be digital now compared to on pieces of paper. So in many application areas, the amount of digital data has exploded. What we saw was with **traditional machine-learning algorithms**, such as logistic regression and linear regression, even as you fed those algorithms more data, it was very difficult to get the performance to keep on going up. So it was as if the traditional learning algorithms like linear regression and logistic regression, they just weren't able to scale with the amount of data we could now feed it and they weren't able to take effective advantage of all this data we had for different applications. What AI researchers started to observe was that if you were to train a **small neural network** on this dataset, then the performance maybe looks like this. If you were to train a **medium-sized neural network**, meaning one with more neurons in it, its performance may look like that. If you were to train a **very large neural network**, meaning one with a lot of these artificial neurons, then for some applications the performance will just keep on going up. So this meant two things, it meant that for a certain class of applications where you do have a lot of data, sometimes you hear the term big data toss around, if you're able to train a very large neural network to take advantage of that huge amount of data you have, then you could attain performance on anything ranging from speech recognition, to image recognition, to natural language processing applications and many more, they just were not possible with earlier generations of learning algorithms. This caused deep learning algorithms to take off, and this too is why faster computer processors, including the rise of GPUs or graphics processor units. This is hardware originally designed to generate nice-looking computer graphics, but turned out to be really powerful for deep learning as well. That was also a major force in allowing deep learning algorithms to become what it is today. \n",
    "That's how neural networks got started, as well as why they took off so quickly in the last several years. Let's now dive more deeply into the details of how neural network actually works. Please go on to the next video.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fa49c",
   "metadata": {},
   "source": [
    "#### 2.1.1.3 Demand Prediction\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-03-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "To illustrate how neural networks work, let's start with an example. We'll use an example from demand prediction in which you look at the product and try to predict, will this product be a top seller or not? Let's take a look. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-03-02.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "In this example, you're selling T-shirts and you would like to know if a particular T-shirt will be a top seller, yes or no, and you have collected data of different t-shirts that were sold at different prices, as well as which ones became a top seller. This type of application is used by retailers today in order to plan better inventory levels as well as marketing campaigns. If you know what's likely to be a top seller, you would plan, for example, to just purchase more of that stock in advance. In this example, **the input feature x is the price of the T-shirt**, and so that's the input to the learning algorithm. If you apply logistic regression to fit a sigmoid function to the data that might look like that then the outputs of your prediction might look like this, $\\frac{1}{1+e^{-wx+b}}$. Previously, we had written this as $f(x)$ as the output of the learning algorithm. In order to set us up to build a neural network, I'm going to switch the terminology a little bit and **use the alphabet $a$ to denote the output of this logistic regression algorithm**. The term $a$ stands for activation, and it's actually a term from neuroscience, and it refers to how much $a$ neuron is sending a high output to other neurons downstream from it. **It turns out that this logistic regression units or this little logistic regression algorithm, can be thought of as a very simplified model of a single neuron in the brain**. Where what the neuron does is it takes us input the price $x$, and then it computes this formula on top, and it outputs the number $a$, which is computed by this formula, and it outputs the probability of this T-shirt being a top seller. Another way to think of a neuron is as a tiny little computer whose only job is to input one number or a few numbers, such as a price, and then to output one number or maybe a few other numbers which in this case is the probability of the T-shirt being a top seller. As I alluded in the previous video, a logistic regression algorithm is much simpler than what any biological neuron in your brain or mine does. Which is why the artificial neural network is such a vastly oversimplified model of the human brain. Even though in practice, as you know, deep learning algorithms do work very well. Given this description of a single neuron, **building a neural network now it just requires taking a bunch of these neurons and wiring them together or putting them together**. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-03-03.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Let's now look at a more complex example of demand prediction. In this example, we're going to have four features to predict whether or not a T-shirt is a top seller. The features are the price of the T-shirt, the shipping costs, the amounts of marketing of that particular T-shirt, as well as the material quality, is this a high-quality, thick cotton versus maybe a lower quality material? \n",
    "\n",
    "Now, you might suspect that whether or not a T-shirt becomes a top seller actually depends on a few factors. First, one is the affordability of this T-shirt. Second is, what's the degree of awareness of this T-shirt that potential buyers have? Third is perceived quality to bias or potential bias saying this is a high-quality T-shirt. \n",
    "\n",
    "What I'm going to do is create one artificial neuron to try to estimate the probability that this T-shirt is perceive as highly affordable. Affordability is mainly a function of price and shipping costs because the total amount of the pay is some of the price plus the shipping costs. We're going to use a little neuron here, a logistic regression unit to input price and shipping costs and predict do people think this is affordable? Second, I'm going to create another artificial neuron here to estimate, is there high awareness of this? Awareness in this case is mainly a function of the marketing of the T-shirt. Finally, going to create another neuron to estimate do people perceive this to be of high quality, and that may mainly be a function of the price of the T-shirt and of the material quality. Price is a factor here because fortunately or unfortunately, if there's a very high priced T-shirt, people will sometimes perceive that to be of high quality because it is very expensive than maybe people think it's going to be of high-quality. Given these estimates of affordability, awareness, and perceived quality we then wire the outputs of these three neurons to another neuron here on the right, that then there's another logistic regression unit. That finally inputs those three numbers and outputs the probability of this t-shirt being a top seller. \n",
    "\n",
    "In the terminology of neural networks, we're going to group these three neurons together into what's called a layer. A layer is a grouping of neurons which takes as input the same or similar features, and that in turn outputs a few numbers together. These three neurons on the left form one layer which is why I drew them on top of each other, and this single neuron on the right is also one layer. The layer on the left has three neurons, so a layer can have multiple neurons or it can also have a single neuron as in the case of this layer on the right. This layer on the right is also called the output layer because the outputs of this final neuron is the output probability predicted by the neural network. In the terminology of neural networks we're also going to call affordability awareness and perceive quality to be **activations**. The term activations comes from biological neurons, and it refers to the degree that the biological neuron is sending a high output value or sending many electrical impulses to other neurons to the downstream from it. \n",
    "These numbers on affordability, awareness, and perceived quality are the activations of these three neurons in this layer, and also this output probability is the activation of this neuron shown here on the right. This particular neural network therefore carries out computations as follows. It inputs four numbers then this layer of the neural network uses those four numbers to compute the new numbers also called activation values. Then the final layer, the output layer of the neural network used those three numbers to compute one number. In a neural network this list of four numbers is also called the **input layer**, and that's just a list of four numbers. Now, there's one simplification I'd like make to this neural network. The way I've described it so far, we had to go through the neurons one at a time and decide what inputs it would take from the previous layer. For example, we said affordability is a function of just price and shipping costs and awareness is a function of just marketing and so on, but if you're building a large neural network it'd be a lot of work to go through and manually decide which neurons should take which features as inputs. The way a neural network is implemented in practice each neuron in a certain layer; say this layer in the middle, will have access to every feature, to every value from the previous layer, from the input layer which is why I'm now drawing arrows from every input feature to every one of these neurons shown here in the middle. You can imagine that if you're trying to predict affordability and it knows what's the price shipping cost marketing and material, may be you'll learn to ignore marketing and material and just figure out through setting the parameters appropriately to only focus on the subset of features that are most relevant to affordability. \n",
    "\n",
    "To further simplify the notation and the description of this neural network **I'm going to take these four input features and write them as a vector $\\vec{x}$, and we're going to view the neural network as having four features that comprise this feature vector $\\vec{x}$. This feature vector is fed to this layer in the middle which then computes three activation values. That is these numbers and these three activation values in turn becomes another vector which is fed to this final output layer that finally outputs the probability of this t-shirt to being a top seller**. That's all a neural network is. It has a few layers where each layer inputs a vector and outputs another vector of numbers. For example, this layer in the middle inputs four numbers x and outputs three numbers corresponding to affordability, awareness, and perceived quality. To add a little bit more terminology, you've seen that this layer is called the **output layer** and this layer is called the **input layer**. To give the layer in the middle a name as well, this layer in the middle is called a **hidden layer**. I know that this is maybe not the best or the most intuitive name but that terminology comes from that's when you have a training set. In a training set, you get to observe both $x$ and $y$. Your data set tells you what is $x$ and what is $y$, and so you get data that tells you what are the correct inputs and the correct outputs. But your dataset doesn't tell you what are the correct values for affordability, awareness, and perceived quality. The correct values for those are hidden. You don't see them in the training set, which is why this layer in the middle is called a hidden layer. \n",
    "\n",
    "I'd like to share with you another way of thinking about neural networks that I've found useful for building my intuition about it. Just let me cover up the left half of this diagram, and see what we're left with. What you see here is that there is a logistic regression algorithm or logistic regression unit that is taking as input, affordability, awareness, and perceived quality of a t-shirt, and using these three features to estimate the probability of the t-shirt being a top seller. This is just logistic regression. But the cool thing about this is rather than using the original features, price, shipping cost, marketing, and so on, it's using a new, maybe better set of features, affordability, awareness, and perceived quality, that are hopefully more predictive of whether or not this t-shirt will be a top seller. One way to think of this neural network is, just logistic regression. But as a version of logistic regression, **they can learn its own features that makes it easier to make accurate predictions**.\n",
    "\n",
    " In fact, you might remember from the previous course, this housing example where we said that if you want to predict the price of the house, you might take the frontage or the width of lots and multiply that by the depth of a lot to construct a more complex feature, $x_1 \\times x_2$, which was the size of the lawn. There we were doing manual feature engineering where we had to look at the features $x_1$ and $x_2$ and decide by hand how to combine them together to come up with better features. **What the neural network does is instead of you needing to manually engineer the features, it can learn, as you'll see later, its own features to make the learning problem easier for itself**. This is what makes neural networks one of the most powerful learning algorithms in the world today. \n",
    "\n",
    " To summarize, a neural network, does this, the **input layer** has a vector of features, four numbers in this example, it is input to the **hidden layer**, which outputs three numbers. I'm going to use a vector to denote this vector of activations that this hidden layer outputs. Then the **output layer** takes its input to three numbers and outputs one number, which would be the **final activation**, or the **final prediction** of the neural network. One note, even though I previously described this neural network as computing affordability, awareness, and perceived quality, one of the really nice properties of a neural network is when you train it from data, **you don't need to go in to explicitly decide what other features**, such as affordability and so on, that **the neural network should compute instead or figure out all by itself what are the features it wants to use in this hidden layer**. That's what makes it such a powerful learning algorithm. You've seen here one example of a neural network and this neural network has a single layer that is a hidden layer. Let's take a look at some other examples of neural networks, specifically, examples with more than one hidden layer. \n",
    "\n",
    " <div align='center'>\n",
    "    <img src='../../static/02-01-01-03-04.png' width=70%/>\n",
    "</div>\n",
    "\n",
    " Here's an example. This neural network has an input feature vector $X$ that is fed to one hidden layer. I'm going to call this the first hidden layer. If this hidden layer has three neurons, it will then output a vector of three activation values. These three numbers can then be input to the second hidden layer. If the second hidden layer has two neurons to logistic units, then this second hidden there will output another vector of now two activation values that maybe goes to the output layer that then outputs the neural network's final prediction. \n",
    "\n",
    " Here's another example. Here's a neural network that it's input goes to the first hidden layer, the output of the first hidden layer goes to the second hidden layer, goes to the third hidden layer, and then finally to the output layer. \n",
    "\n",
    " When you're building your own neural network, one of the decisions you need to make is **how many hidden layers do you want and how many neurons do you want each hidden layer to have**. This question of how many hidden layers and how many neurons per hidden layer is a question of the architecture of the neural network. You'll learn later in this course some tips for choosing an appropriate architecture for a neural network. But choosing the right number of hidden layers and number of hidden units per layer can have an impact on the performance of a learning algorithm as well. Later in this course, you'll learn how to choose a good architecture for your neural network as well. By the way, in some of the literature, you see **this type of neural network with multiple layers like this called a multilayer perceptron**. If you see that, that just refers to a neural network that looks like what you're seeing here on the slide.\n",
    " \n",
    "  That's a neural network. I know we went through a lot in this video. Thank you for sticking with me. But you now know how a neural network works. In the next video, let's take a look at how these ideas can be applied to other applications as well. In particular, we'll take a look at the computer vision application of face recognition. Let's go on to the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ae888",
   "metadata": {},
   "source": [
    "#### 2.1.1.4 Example: Recognizing Images\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-04-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "In the last video, you saw how a neural network works in a demand prediction example. Let's take a look at how you can apply a similar type of idea to computer vision application. Let's dive in. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-04-02.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "If you're building a face recognition application, you might want to train a neural network that takes as input a picture like this and outputs the identity of the person in the picture. This image is 1,000 by 1,000 pixels. Its representation in the computer is actually as **thousand by thousand grid**, or also called **thousand by thousand matrix of pixel intensity values**. In this example, my pixel intensity values or pixel brightness values, goes from 0-255 and so 197 here would be the brightness of the pixel in the very upper left of the image, 185 is brightness of the pixel, one pixel over, and so on down to 214 would be the lower right corner of this image. If you were to take these pixel intensity values and unroll them into a vector, you end up with a list or a vector of a million pixel intensity values. One million because thousand by thousand square gives you a million numbers. **The face recognition problem is, can you train a neural network that takes as input a feature vector with a million pixel brightness values and outputs the identity of the person in the picture**. This is how you might build a neural network to carry out this task. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-04-03.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "The input image X is fed to this layer of neurons. This is the first hidden layer, which then extract some features. The output of this first hidden layer is fed to a second hidden layer and that output is fed to a third layer and then finally to the output layer, which then estimates, say the probability of this being a particular person. One interesting thing would be if you look at a neural network that's been trained on a lot of images of faces and to try to visualize what are these hidden layers, trying to compute. It turns out that when you train a system like this on a lot of pictures of faces and you peer at the different neurons in the hidden layers to figure out what they may be computing this is what you might find. \n",
    "\n",
    "In the first hidden layer, you might find one neuron that is looking for the low vertical line or a vertical edge like that. A second neuron looking for a oriented line or oriented edge like that. The third neuron looking for a line at that orientation, and so on. In the earliest layers of a neural network, you might find that the neurons are looking for very short lines or very short edges in the image. \n",
    "\n",
    "If you look at the next hidden layer, you find that these neurons might learn to group together lots of little short lines and little short edge segments in order to look for parts of faces. For example, each of these little square boxes is a visualization of what that neuron is trying to detect. This first neuron looks like it's trying to detect the presence or absence of an eye in a certain position of the image. The second neuron, looks like it's trying to detect like a corner of a nose and maybe this neuron over here is trying to detect the bottom of an ear. \n",
    "\n",
    "Then as you look at the next hidden layer in this example, the neural network is aggregating different parts of faces to then try to detect presence or absence of larger, coarser face shapes. Then finally, detecting how much the face corresponds to different face shapes creates a rich set of features that then helps the output layer try to determine the identity of the person picture. \n",
    "\n",
    "A remarkable thing about the neural network is **you can learn these feature detectors at the different hidden layers all by itself**. In this example, no one ever told it to look for short little edges in the first layer, and eyes and noses and face parts in the second layer and then more complete face shapes at the third layer. The neural network is able to figure out these things all by itself from data. Just one note, in this visualization, the neurons in the first hidden layer are shown looking at relatively small windows to look for these edges. In the second hidden layer is looking at bigger window, and the third hidden layer is looking at even bigger window. These little neurons visualizations actually correspond to differently sized regions in the image. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-04-04.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Just for fun, let's see what happens if you were to train this neural network on a different dataset, say on lots of pictures of cars, picture on the side. The same learning algorithm is asked to detect cars, will then learn edges in the first layer. Pretty similar but then they'll learn to detect parts of cars in the second hidden layer and then more complete car shapes in the third hidden layer. **Just by feeding it different data, the neural network automatically learns to detect very different features so as to try to make the predictions of car detection or person recognition or whether there's a particular given task that is trained on**. \n",
    "\n",
    "That's how a neural network works for computer vision application. In fact, later this week, you'll see how you can build a neural network yourself and apply it to a handwritten digit recognition application. So far we've been going over the description of intuitions of neural networks to give you a feel for how they work. In the next video, let's look more deeply into the concrete mathematics and a concrete implementation of details of how you actually build one or more layers of a neural network, and therefore how you can implement one of these things yourself. Let's go on to the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a01cd",
   "metadata": {},
   "source": [
    "#### 2.1.1.5 [IMPORTANT] Have questions, issues or ideas? Join our community on Discourse!\n",
    "\n",
    "Hello!\n",
    "\n",
    "We’ve created a Discourse community for you to:\n",
    "\n",
    "- Ask for help on assignments and other course content\n",
    "- Discuss course topics\n",
    "- Share your knowledge with other learners\n",
    "- Build your network, and\n",
    "- Find out about exciting DeepLearning.AI news, events and competitions.\n",
    "\n",
    "New to Discourse (community.deeplearning.ai)? Don’t worry! We created this [User Guide](https://docs.google.com/document/d/1TgGE0t5J83md2HnN-FymwX8P9TnUdJHvsjBOnLrfWO4/edit) for you. Do ensure to abide by the [Code of Conduct](https://docs.google.com/document/d/1UoKfjNYw33cSu2msPMc9SjytrK0IFuKy7O6kLnh8vng/edit) guidelines.\n",
    "\n",
    "Please use [this invite link](https://community.deeplearning.ai/invites/osckK8Sjcc) to register on Discourse (url: community.deeplearning.ai). After joining, you can go to this specialization's [subcategory here](https://community.deeplearning.ai/login). You can bookmark it so you can revisit it anytime you have questions.\n",
    "\n",
    "Having trouble accessing the Discourse site? Fill out this [form](https://docs.google.com/forms/d/e/1FAIpQLSeaLh4yDVyewvthP2ThVaz0daU9fACkihRlSfT-CMUw12Gidw/viewform?usp=send_form) to explain your issue and we will get back to you with an alternative.\n",
    "\n",
    "We hope to see you on Discourse soon!\n",
    "\n",
    "- The DeepLearning.AI team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81bdade",
   "metadata": {},
   "source": [
    "#### 2.1.1.6 Practice quiz: Neural network intuition\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-06-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-06-02.png' width=70%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72894925",
   "metadata": {},
   "source": [
    "### 2.1.2 Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1a5d5",
   "metadata": {},
   "source": [
    "#### 2.1.2.1 Neural network layer\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-01-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "The fundamental building block of most modern neural networks is a layer of neurons. In this video, you'll learn how to construct a layer of neurons and once you have that down, you'd be able to take those building blocks and put them together to form a large neural network. Let's take a look at how a layer of neurons works. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-01-02.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Here's the example we had from the demand prediction example where we had four input features that were set to this layer of three neurons in the hidden layer that then sends its output to this output layer with just one neuron. \n",
    "\n",
    "Let's zoom in to the hidden layer to look at its computations. This hidden layer inputs four numbers and these four numbers are inputs to each of three neurons. Each of these three neurons is just implementing a little logistic regression unit or a little bit logistic regression function. Take this first neuron. It has two parameters, $\\vec{w}$ and $b$. In fact, to denote that, this is the first hidden unit, I'm going to subscript this as $\\vec{w}_1$, $b_1$. What it does is I'll **output some activation value $a$**, which is $g(\\vec{w} \\cdot \\vec{x} + b)$, where this is the familiar $z$ value that you have learned about in logistic regression in the previous course, and $g(z)$ is the familiar logistic function, $\\frac{1}{1+e^{-(z)}}$. Maybe this ends up being a number $0.3$ and that's the activation value $a$ of the first neuron. To denote that this is the first neuron, I'm also going to add a subscript $a_1$ over here, and so $a_1$ may be a number like $0.3$, just a $0.3$ chance of this being highly affordable based on the input features. Now let's look at the second neuron. The second neuron has parameters $\\vec{w}_2$ and $b_2$, and these $\\vec{w}$, $b$ or $\\vec{w}_2$, $b_2$ are the parameters of the second logistic unit. It computes $a_2=g(\\vec{w}_2 \\cdot \\vec{x} + b_2)$ and this may be some other number, say $0.7$. Because in this example, there's a $0.7$ chance that we think the potential buyers will be aware of this t-shirt. Similarly, the third neuron has a third set of parameters $\\vec{w}_3$, $b_3$. Similarly, it computes an activation value $a_3 = g(\\vec{w}_3 \\cdot \\vec{x} + b_3)$ and that may be say, $0.2$. In this example, these three neurons output $0.3$, $0.7$, and $0.2$, and this vector of three numbers becomes the vector of activation values $\\vec{a}$, that is then passed to the final output layer of this neural network. \n",
    "\n",
    "Now, when you build neural networks with multiple layers, it'll be useful to give the layers different numbers. By convention, this layer is called **layer 1** of the neural network and this layer is called **layer 2** of the neural network. **The input layer is also sometimes called layer 0** and today, there are neural networks that can have dozens or even hundreds of layers. **But in order to introduce notation to help us distinguish between the different layers, I'm going to use superscript square bracket 1 to index into different layers**. In particular, $\\vec{a}^{[1]}$, I'm going to use, that's a notation to denote the output of layer 1 of this hidden layer of this neural network, and similarly, $\\vec{w}_1$, $b_1$ here are the parameters of the first unit in layer 1 of the neural network, so I'm also going to add a superscript in square brackets 1 here, and $\\vec{w}_2$, $b_2$ are the parameters of the second hidden unit or the second hidden neuron in layer 1. Its parameters are also denoted here, $\\vec{w}^{[1]}_2$ like so. Similarly, I can add superscripts square brackets like so to denote that these are the activation values of the hidden units of layer 1 of this neural network. I know maybe this notation is getting a little bit cluttered. But the thing to remember is whenever you see this superscript square bracket 1, that just refers to a quantity that is associated with layer 1 of the neural network. If you see superscript square bracket 2, that refers to a quantity associated with layer 2 of the neural network and similarly for other layers as well, including layer 3, layer 4 and so on for neural networks with more layers. \n",
    "\n",
    "That's the computation of layer 1 of this neural network. Its output is this activation vector, $\\vec{a}^{[1]}$ and I'm going to copy this over here because this output $\\vec{a}^{[1]}$ becomes the input to layer 2. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-01-03.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Now let's zoom into the computation of layer 2 of this neural network, which is also the output layer. The input to layer 2 is the output of layer 1, so $\\vec{a}^{[1]}$ is this vector $0.3$, $0.7$, $0.2$ that we just computed on the previous part of this slide. Because the output layer has just a single neuron, all it does is it computes $\\vec{a}^{[1]}$ that is the output of this first and only neuron, as $g$, the sigmoid function applied to $\\vec{w} _1$ in a product with $\\vec{a}^{[1]}$, so this is the input into this layer, and then plus $b_1$, $a_1=g(\\vec{w}_{1} \\cdot \\vec{a}^{[1]} + b_1)$. Here, this is the quantity $z$ that you familiar with and $g$ as before is the sigmoid function that you apply to this. If this results in a number, say $0.84$, then that becomes the output layer of the neural network. In this example, because the output layer has just a single neuron, this output is just a scalar, is a single number rather than a vector of numbers. Sticking with our notational convention from before, we're going to use a superscript in square brackets 2, to denote the quantities associated with layer 2 of this neural network, so $\\vec{a}^{[2]}$ is the output of this layer, and so I'm going to also copy this here as the final output of the neural network. To make the notation consistent, you can also add these superscripts square bracket 2s to denote that these are the parameters and activation values associated with layer 2 of the neural network. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-01-04.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Once the neural network has computed $\\vec{a}^{[2]}$, there's one final optional step that you can choose to implement or not, which is if you want a binary prediction, 1 or 0, is this a top seller? Yes or no? As you can take the number $\\vec{a}^{[2]}_1$, and this is the number $0.84$ that we computed, and threshold this at $0.5$. If it's greater than $0.5$, you can predict $y$ hat equals $1$ and if it is less than $0.5$, then predict your $y$ hat equals $0$. We saw this thresholding as well when you learned about logistic regression in the first course of the specialization. If you wish, this then gives you the final prediction $\\hat{y}$ as either one or zero, if you don't want just the probability of it being a top seller. \n",
    "\n",
    "So that's how a neural network works. Every layer inputs a vector of numbers and applies a bunch of logistic regression units to it, and then computes another vector of numbers that then gets passed from layer to layer until you get to the final output layers computation, which is the prediction of the neural network. Then you can either threshold at $0.5$ or not to come up with the final prediction. With that, let's go on to use this foundation we've built now to look at some even more complex, even larger neural network models. I hope that by seeing more examples, this concept of layers and how to put them together to build a neural network will become even clearer. So let's go on to the next video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a6314",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-02-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "In the last video, you learned about the neural network layer and how that takes this inputs a vector of numbers and in turn, outputs another vector of numbers. In this video, let's use that layer to build a more complex neural network. Through this, I hope that the notation that we're using for neural networks will become clearer and more concrete as well. Let's take a look. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-02-02.png' width=70%>\n",
    "</div>\n",
    "\n",
    "This is the running example that I'm going to use throughout this video as an example of a more complex neural network. This network has four layers, not counting the input layer, which is also called Layer 0, where layers 1, 2, and 3 are hidden layers, and Layer 4 is the output layer, and Layer 0, as usual, is the input layer. By convention, **when we say that a neural network has four layers, that includes all the hidden layers in the output layer, but we don't count the input layer**. This is a neural network with four layers in the conventional way of counting layers in the network. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-02-03.png' width=70%>\n",
    "</div>\n",
    "\n",
    "Let's zoom in to Layer 3, which is the third and final hidden layer to look at the computations of that layer. Layer 3 inputs a vector, a superscript square bracket 2 that was computed by the previous layer, and it outputs $\\vec{a}^{[3]}$, which is another vector. What is the computation that Layer 3 does in order to go from $\\vec{a}^{[2]}$ to $\\vec{a}^{[3]}$? If it has three neurons or we call it three hidden units, then it has parameters $\\vec{w}_{1}$, $b_1$, $\\vec{w}_{2}$, $b_2$, and $\\vec{w}_{3}$, $b_3$ and it computes $a^{[3]}_{1}$ equals sigmoid of $\\vec{w}^{[3]}_{1}$ product with this input to the layer, plus $b_1$, and it computes $a^{[3]}_{2}$ equals sigmoid of $\\vec{w}^{[3]}_{1}$ product with, again, $\\vec{a}^{[2]}$, the input to the layer plus $b_2$ and so on to get $a^{[3]}_{3}$. Then the output of this layer is a vector comprising $a_1$, $a_2$, and $a_3$. Again, by convention, if we want to more explicitly denote that all of these are quantities associated with Layer 3 then we add in all of these superscript square brackets 3 here, to denote that these parameters $\\vec{w}$ and $b$ are the parameters associated with neurons in Layer 3 and that these activations are activations with Layer 3. Notice that this term here is $\\vec{w}^{[3]}_{1}$, meaning the parameters associated with Layer 3. product with a superscript square bracket 2, which was the output of Layer 2, which became the input to Layer 3. That's why it has a 3 here because it's a parameter associator of Layer 3 dot product with, and there's a 2 there because is the output of Layer 2. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-02-04.png' width=70%>\n",
    "</div>\n",
    "\n",
    "Now, let's just do a quick double check on our understanding of this. I'm going to hide the superscripts and subscripts associated with the second neuron and without rewinding this video, go ahead and rewind if you want, but prefer you not. But without rewinding this video, are you able to think through what are the missing superscripts and subscripts in this equation and fill them in yourself? Once you take a look at the end video quiz and see if you can figure out what are the appropriate superscripts and subscripts for this equation over here. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-02-05.png' width=70%>\n",
    "</div>\n",
    "\n",
    "If you chose the 1st option, then you got it right! The activation of the 2nd neuron at layer 3 is denoted by $a^{[3]}_{2}$. To apply the activation function $g$, lets use the parameters of this same neuron. So $\\vec{w}$ and $b$ will have the same subscript 2 and superscript square bracket 3. The input features will be the output vector from the previous layer, which is layer 2. So that will be the vector $\\vec{a}$ superscript 2. The second option is using vector $\\vec{a}^{[3]}$ which is not the output vector from the previous layer. The input to this layer is $\\vec{a}^{[2]}$. And the 3rd option has $a^{[2]}_{2}$ as input, which is a single number rather than the vector Because recall that the correct input is a vector $\\vec{a}^{[2]}$, with the little arrow on top, and not a single number.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-02-06.png' width=70%>\n",
    "</div>\n",
    "\n",
    "To recap, $\\vec{a}^{[3]}$ is activation associated with Layer 3 for the second neuron, hence it's $a^{[3]}_{2}$. There is a parameter associated with the third layer for the second neuron. this is $\\vec{a}^{[2]}$, same as above and then plus $b^{[3]}_2$. Hopefully, that makes sense. Just the more general form of this equation for an arbitrary Layer $l$ and for an arbitrary unit $j$, which is that $a$ deactivation output of layer $l$, unit $j$, like $a^{[3]}_{2}$, that's going to be the sigmoid function applied to this term, which is the weight vector of layer $l$, such as Layer 3 for the $j\\_ th$ unit so there's a 2 again in the example above. And so that's dot-producted with $\\vec{a}$ deactivation value of, and notice this is not $l$, this is $l - 1$, like $\\vec{a}_{2}$ above here. Because you're dot-producting with the output from the previous layer, and then plus b, the parameter for this layer for that unit $j$. This gives you the activation of layer $l$ unit $j$, where the superscript in square brackets $l$ denotes layer $l$ and a subscript $j$ denotes unit $j$. When building neural networks, unit $j$ refers to the $j\\_ th$ neuron, so we use those terms a little bit interchangeably where each unit is a single neuron in the layer. $g$ here is the sigmoid function. In the context of a neural network, $g$ has another name, which is also called the **activation function**, because $g$ outputs this activation value. When I say activation function, I mean this function $g$ here. So far, the only activation function you've seen, this is a sigmoid function. But next week, we'll look at when other functions, then the sigmoid function can be plugged in place of $g$ as well. **The activation function is just that function that outputs these activation values**. Just one last piece of notation. In order to make all this notation consistent, I'm also going to give the input vector $\\vec{x}$ and another name which is $\\vec{a}^{[0]}$. So this way, the same equation also works for the first layer, where when $l$ is equal to 1, the activations of the first layer, that is $\\vec{a}^{[1]}$, would be the sigmoid times the weights dot-product with $\\vec{a}^{[0]}$, which is just this input feature vector $\\vec{x}$. With this notation, you now know how to compute the activation values of any layer in a neural network as a function of the parameters as well as the activations of the previous layer. \n",
    "\n",
    "You now know how to compute the activations of any layer given the activations of the previous layer. Let's put this into an inference algorithm for a neural network. In other words, how to get a neural network to make predictions. Let's go see that in the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58932dd7",
   "metadata": {},
   "source": [
    "#### 2.1.2.3 Inference: making predictions (forward propagation)\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-03-01.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "Let's take what we've learned and put it together into an algorithm to let your neural network make inferences or make predictions. This will be an algorithm called **forward propagation**. Let's take a look. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-03-02.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "I'm going to use as a motivating example, handwritten digit recognition. And for simplicity we are just going to distinguish between the handwritten digits zero and one. So it's just a binary classification problem where we're going to input an image and classify, is this the digit zero or the digit one? And you get to play with this yourself later this week in the practice lab as well. \n",
    "\n",
    "For the example of the slide, I'm going to use an eight by eight image. And so this image of a one is this grid or matrix of eight by eight or 64 pixel intensity values where 255 denotes a bright white pixel and zero would denote a black pixel. And different numbers are different shades of gray in between the shades of black and white. Given these 64 input features, we're going to use the neural network with two hidden layers, where the first hidden layer has 25 neurons or 25 units. Second hidden layer has 15 neurons or 15 units. And then finally the output layer or outputs unit, what's the chance of this being 1 versus 0?. So let's step through the sequence of computations that in your neural network will need to make to go from the input $\\vec{x}$, this eight by eight or 64 numbers to the predicted probability $\\vec{a}^{[3]}$. The first computation is to go from $\\vec{x}$ to $\\vec{a}^{[1]}$, and that's what the first layer of the first hidden layer does. It carries out a computation of $\\vec{a}^{[1]}$ equals this formula on the right. Notice that a one has 25 numbers because this hidden layer has 25 units. Which is why the parameters go from $\\vec{w}^{[1]}_{1}$ through $\\vec{w}^{[1]}_{25}$ as well as $b^{[1]}_{1}$ through $b^{[1]}_{25}$. And I've written $\\vec{x}$ here but I could also have written $\\vec{a}^{[0]}$ here because by convention the activation of layer zero, that is $\\vec{a}^{[0]}$ is equal to the input feature value $\\vec{x}$. So let's just compute $\\vec{a}^{[1]}$. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-03-03.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "The next step is to compute $\\vec{a}^{[2]}$. Looking at the second hidden layer, it then carries out this computation where $\\vec{a}^{[2]}$ is a function of $\\vec{a}^{[1]}$ and it's computed as the sigmoid activation function applied to $\\vec{w}$ dot product $\\vec{a}^{[1]}$ plus the corresponding value of $b$. Notice that layer two has 15 neurons or 15 units, which is why the parameters Here run from $\\vec{w}^{[2]}_{1}$ through $\\vec{w}^{[2]}_{15}$ and $b^{[2]}_{1}$ through $b^{[2]}_{15}$. Now we've computed $\\vec{a}^{[2]}$. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-02-03-04.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "The Final step is then to compute $\\vec{a}^{[3]}$ and we do so using a very similar computation. Only now, this third layer, the output layer has just one unit, which is why there's just one output here. So $\\vec{a}^{[3]}$ is just a scalar. And finally you can optionally take $\\vec{a}^{[3]}$  and threshold it at 0.5 to come up with a binary classification label. Is this the digit 1? Yes or no? \n",
    "\n",
    "So the sequence of computations first takes $\\vec{x}$ and then computes $\\vec{a}^{[1]}$, and then computes $\\vec{a}^{[2]}$, and then computes $\\vec{a}^{[3]}$, which is also the output of the neural networks. You can also write that as $f(x)$. So remember when we learned about linear regression and logistic regression, we use $f(x)$ to denote the output of linear regression or logistic regression. So we can also use $f(x)$ to denote the function computed by the neural network as a function of $x$. Because this computation goes from left to right, you start from $\\vec{x}$ and compute $\\vec{a}^{[1]}$, then $\\vec{a}^{[2]}$, then $\\vec{a}^{[3]}$. This algorithm is also called **forward propagation** because you're propagating the activations of the neurons. So you're making these computations in the forward direction from left to right. And this is in contrast to a different algorithm called **backward propagation** or **back propagation**, which is used for learning. And that's something you learn about next week. And by the way, this type of neural network architecture where you have more hidden units initially and then the number of hidden units decreases as you get closer to the output layer. There's also a pretty typical choice when choosing neural network architectures. And you see more examples of this in the practice lab as well. \n",
    "\n",
    "So that's neural network inference using the forward propagation algorithm. And with this, you'd be able to download the parameters of a neural network that someone else had trained and posted on the Internet. And you'd be able to carry out inference on your new data using their neural network. Now that you've seen the math and the algorithm, let's take a look at how you can actually implement this in tensorflow. Specifically, let's take a look at this in the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798004f",
   "metadata": {},
   "source": [
    "#### 2.1.2.4 Lab: Neurons and Layers\n",
    "\n",
    "In this lab we will explore the inner workings of neurons/units and layers. In particular, the lab will draw parallels to the models you have mastered in Course 1, the regression/linear model and the logistic model. The lab will introduce Tensorflow and demonstrate how these models are implemented in that framework.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../../static/02-01-01-02-04.png' width=70%/>\n",
    "</div>\n",
    "\n",
    "##### Packages\n",
    "**Tensorflow and Keras**\n",
    "Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by François Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9316e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeaenSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
