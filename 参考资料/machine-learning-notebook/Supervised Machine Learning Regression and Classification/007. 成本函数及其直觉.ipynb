{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b1421a",
   "metadata": {},
   "source": [
    "# Linear Regression with One Variable\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "In order to implement linear regression, the first key step is for us to define something called a cost function. The cost function will tell us how well the model is doing so that we can try to get it to better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0353d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"images/007-01.png\" width=\"1000\"><center/>\n",
    "<figure/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84422db2",
   "metadata": {},
   "source": [
    "Recall that you have a training set that contains input featrues $x$ and output targets $y$, and the model you're going to use to fit this training set is this linear function $f_{w,b}(x)=wx+b$. To introduce a little bit more terminology, the $w$ and $b$ are called the **parameters** of the model. In machine learning, parameters of a model are the variables you can adjust during training in order to imporve the model. Sometimes you also hear the parameters $w$ and $b$ referred to as **coefficients** or as **weights**.\n",
    "\n",
    "Now let's take a look at what these parameters $w$ and $b$ do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22e7f8",
   "metadata": {},
   "source": [
    "Depending on the values you've chosen for $w$ and $b$, you get different function $f$ of $x$, which generates a different line on the graph.\n",
    "\n",
    "We are going to take a look at some plots of $f(x)$ on a chart. Maybe you're already familiar with drawing lines on charts, but even if this is a review for you, I hope this will help you build intuition on how $w$ and $b$, the parameters, determine $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b7840",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"images/007-02.png\" width=\"1000\"><center/>\n",
    "<figure/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e604f2",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"images/007-03.png\" width=\"1000\"><center/>\n",
    "<figure/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d938eeaf",
   "metadata": {},
   "source": [
    "Recall that you have a training set like the one shown here. With linear regression, what you want to do is to choose values for the parameters $w$ and $b$, so that the straight line you get from the function $f$ somehow fits the data well, like  maybe this line shown here. And when I say that the line fits the data visually, you can think of this to mean that the line defined by $f$ is roughly passing through or somewhat close to the training examples as compared to other possible line that are not as close to these points.\n",
    "\n",
    "And just to remind you of some notation, a training example like this point here is defined by $(x^{(i)},y^{(i)})$, where $y$ is the target. For given input $x^{(i)}$, the function $f$ also makes a predicted value for $y$, and the value that it predicts for $y$ is $\\hat{y}$, shown here. For our choice of model, $f_{w,b}(x^{(i)})=wx^{(i)}+b$\n",
    "\n",
    "So now the question is, how do you find values for $w$ and $b$ so that the **prediction $\\hat{y}^{(i)}$ is close to the true target $y^{(i)}$ for many or mayble all training examples $(x^{(i)},y^{(i)})$**. To answer that quesion, let's first take a look at how to measure how well in a line fits the training data.\n",
    "\n",
    "To answer that question, let's first take a look at how to measure how well a line fits the training data. To do that, we're going to construct our cost function. The cost function takes the prediction $\\hat{y}$ and compares to the target $y$ by taking $\\hat{y} - y$, this difference is called the **error**. We're measuring how far off the prediction is from the target. Next, let's compute the square of this error. Also, we're going to compute this term for different training example $i$ in the training set. So when measuring the error, for example $i$, we'll compute this squared error term. Finally, we want to measure the error across the entire training set. In particular, let's sum up the squared errors like this $\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^{2}$. Remember that $m$ is the number of training examples.\n",
    "\n",
    "Notice that if we have more training examples, $m$ is larger and your cost function will calculate a bigger number since it's summing over more examples. So to build a cost function that doesn't automatically get bigger as the training set size gets larger, by convention, we'll compute the **average squared error** instead of the total squared error, and we do that by dividing by $m$ like this: $\\frac{1}{m} \\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^{2}$\n",
    "\n",
    "Okay, we're nearly there. Just one last thing. By convention, the cost function that machine learning people use actually divides by 2 times $m$. The extra division by 2 is just meant to make some of our later calculations a little bit neater, but the cost function still works whether your include this division by 2 or not.\n",
    "\n",
    "So this expression right here is the cost function, and we're going to write $J(w,b)$ to refer to the cost function. This is also called the **squared error cost function**.\n",
    "\n",
    "Eventually, we're going to want to find values of $w$ and $b$ that make the cost function small, but before going there, let's first gain more intuition about what of $J(w,b)$ is really computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316fbb8f",
   "metadata": {},
   "source": [
    "## Cost Function Intuition\n",
    "\n",
    "We've seen the mathematical definition of the cost function. Now let's build some **intuition** about what the cost function is really doing.\n",
    "\n",
    "We'll through one example to see how the cost function can be used to find the best paramters for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd4bb3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"images/007-04.png\" width=\"1000\"></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c1345",
   "metadata": {},
   "source": [
    "To recap, here's what we've seen about the **cost function** so far.\n",
    "\n",
    "You want to fit a straight line to the training data so you have this **model** $f_{w,b}(x)=wx+b$ and here the model's **paramters** are $w$ and $b$. Now depending on the values chosen for these paramters you get different straight lines like this and you want to find values for $w$ and $b$ so that the straight line fits the training data well. \n",
    "\n",
    "To measure how well a choice of $w$ and $b$ fits the training data you have a **cost function** $J$ and what the cost function $J$ does is it measures the difference between the model's predictions and the actual true values for $y$.\n",
    "\n",
    "What you see later is that linear regression well try to ind values for $w$ and $b$ that make $J(w,b)$ as small as possible. In math we write it like this $minimize_{w,b} J(w,b)$\n",
    "\n",
    "So now in order for us to better visualize the cost function $J$ let's work of a simplified version of the linear regression model.\n",
    "\n",
    "We're going to use the model $f_w(x)=wx$. So you now have just one parameter $w$ and your cost function $J$ looks similar to what it was before, taking the difference and squaring it except that now $f=wx_i$ and $J$ now a function of just $w$ and the goal becomes a little bit different as well because you have just one parameter $w$.\n",
    "\n",
    "So with this simplified model the goal is to find the value for $w$ that minimizes $J(w)$.\n",
    "\n",
    "Now using this simplified model let's see how the cost function changes as you choose different values for the parameter $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03507f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center><img src=\"images/007-05.png\" width=\"1000\"></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f509fa",
   "metadata": {},
   "source": [
    "In particular let's look at graphs of the model $f_w(x)$ and the cost function $J$.\n",
    "\n",
    "First notice that for $f_w$ when the parameter $w$ is fixed that is always a constant value then $f_w$ is only a function of $x$ which means that the estimated value of $y$ depends on the value of the input $x$. In contrast looking to the right, the cost function $J$ is a function of $w$ where $w$ controls the slope of the line defined by $f_w$. So the cost define by $J$ depends on a parameter, in this case the parameter $w$. So let's go ahead and plot these functions $f_w(x)$ and the cost function $J(w)$ side by side so you can see how they are related.\n",
    "\n",
    "We'll start to the model that is the function $f_w(x)$ on the left. Here the input featrue $x$ is on the horizontal axis and the output value $y$ is on the vertical axis. Here's a plot of three points representing the training set at position $(1,1)$,$(2,2)$,$(3,3)$. Let;s pick a value for $w$ say $w$ is 1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
